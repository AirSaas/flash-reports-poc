import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import Anthropic from "npm:@anthropic-ai/sdk"
import { corsHeaders, handleCors } from "../_shared/cors.ts"
import { getSupabaseClient } from "../_shared/supabase.ts"
import { getAnthropicClient } from "../_shared/anthropic.ts"

const EVALUATION_THRESHOLD = 65

// JSON Schema for structured evaluation output
// Note: Claude structured outputs don't support min/max on integers, so constraints are in the prompt
const EVALUATION_SCHEMA = {
  type: "object",
  properties: {
    score: { type: "integer" },
    completeness: { type: "integer" },
    accuracy: { type: "integer" },
    formatting: { type: "integer" },
    projectsFound: { type: "integer" },
    projectsExpected: { type: "integer" },
    issues: { type: "array", items: { type: "string" } },
    accuracyIssues: { type: "array", items: { type: "string" } },
    emptyFields: { type: "array", items: { type: "string" } },
    recommendation: { type: "string", enum: ["pass", "regenerate"] }
  },
  required: ["score", "completeness", "accuracy", "formatting", "projectsFound", "projectsExpected", "issues", "accuracyIssues", "emptyFields", "recommendation"],
  additionalProperties: false
}

const EVALUATION_PROMPT_PPTX = `You are an expert quality evaluator for project portfolio reports generated as PowerPoint presentations.

## Context
This is a "Flash Report" — a project portfolio status report generated from AirSaas project management data. Each report contains slides for multiple projects, showing key metrics like project name, owner, status, budget, timeline, risks, and progress indicators. The PPTX was generated by Claude using a user-provided template.

## Your Task
Use code execution with python-pptx to inspect the uploaded PPTX file. Extract text from each slide, count projects, and identify any issues.

## Scoring Criteria (Total: 100 points)

### Content Structure (0-40 points) — return as "completeness"
- Report has a clear structure with a title/cover slide and per-project slides
- Each project section contains identifiable data fields (name, owner, status, dates, etc.)
- Information is organized logically and consistently across projects
- No placeholder text like "{{field_name}}", "[PLACEHOLDER]", "TBD", "N/A", or "Lorem ipsum" remaining
- All expected projects are present (compare projectsFound vs projectsExpected)
- Deduct 5 points per missing project

### Data Quality (0-40 points) — return as "accuracy"
- Fields contain specific, concrete values rather than generic or templated text
- Project names, owner names, and dates look like real data (not "Project 1", "John Doe", "01/01/2025")
- Numbers are realistic (budgets, percentages, durations)
- Status indicators and RAG (Red/Amber/Green) values are present where expected
- No fields are suspiciously identical across different projects (copy-paste errors)
- Text content is in the expected language (French for AirSaas reports)

### Formatting (0-20 points) — return as "formatting"
- Professional presentation layout matching the template style
- Consistent styling across all project slides (fonts, colors, alignment)
- Text is readable — not cut off, overlapping, or overflowing text boxes
- Tables and charts (if any) are properly rendered
- Visual hierarchy is clear (titles > subtitles > body content)

## Output Requirements
- score: Total score (0-100), MUST equal completeness + accuracy + formatting
- completeness: Content structure score (0-40)
- accuracy: Data quality score (0-40)
- formatting: Visual formatting score (0-20)
- projectsFound: Number of distinct projects identified in the presentation
- projectsExpected: Use the number provided in the user message
- issues: Array of general issues found (empty array if none)
- accuracyIssues: Array of specific data quality problems (e.g., "Slide 3: budget field shows '{{budget}}' placeholder")
- emptyFields: Array of fields that appear empty, blank, or contain placeholder values (e.g., "Slide 2: 'Owner' field is empty")
- recommendation: "pass" if score >= 65, "regenerate" if score < 65

## Instructions
1. Use code execution to open the PPTX with python-pptx
2. Iterate through all slides, extracting text from every shape and table cell
3. Count the number of distinct project sections
4. Search for placeholder patterns: {{...}}, [...], TBD, N/A, Lorem ipsum, empty strings
5. Verify each project has populated key fields (name, status, owner at minimum)
6. Calculate scores for each category
7. Return your evaluation as JSON`

const EVALUATION_PROMPT_PDF = `You are an expert quality evaluator for project portfolio reports generated as PDF documents.

## Context
This is a "Flash Report" — a project portfolio status report generated from AirSaas project management data. The report contains pages/sections for multiple projects, showing key metrics like project name, owner, status, budget, timeline, risks, and progress indicators. The PDF was generated from an HTML template populated with real project data.

## Your Task
Analyze the visual content of each page in this PDF document. Examine text, tables, charts, and layout carefully.

## Scoring Criteria (Total: 100 points)

### Content Structure (0-40 points) — return as "completeness"
- Report has a clear structure with a title/cover page and per-project sections
- Each project section contains identifiable data fields (name, owner, status, dates, etc.)
- Information is organized logically and consistently across projects
- No placeholder text like "{{field_name}}", "[PLACEHOLDER]", "TBD", "N/A", or "Lorem ipsum" remaining
- All expected projects are present (compare projectsFound vs projectsExpected)
- Deduct 5 points per missing project

### Data Quality (0-40 points) — return as "accuracy"
- Fields contain specific, concrete values rather than generic or templated text
- Project names, owner names, and dates look like real data (not "Project 1", "John Doe", "01/01/2025")
- Numbers are realistic (budgets, percentages, durations)
- Status indicators and RAG (Red/Amber/Green) values are present where expected
- No fields are suspiciously identical across different projects (copy-paste errors)
- Text content is in the expected language (French for AirSaas reports)

### Formatting (0-20 points) — return as "formatting"
- Clean, professional page layout respecting margins and spacing
- Consistent styling across all project sections (fonts, colors, alignment)
- Text is fully readable — not cut off, overlapping, or running outside content areas
- Tables are properly formatted with aligned columns and visible borders
- Visual hierarchy is clear (titles > subtitles > body content)
- No broken images or rendering artifacts

## Output Requirements
- score: Total score (0-100), MUST equal completeness + accuracy + formatting
- completeness: Content structure score (0-40)
- accuracy: Data quality score (0-40)
- formatting: Visual formatting score (0-20)
- projectsFound: Number of distinct projects identified in the document
- projectsExpected: Use the number provided in the user message
- issues: Array of general issues found (empty array if none)
- accuracyIssues: Array of specific data quality problems (e.g., "Page 3: budget field shows '{{budget}}' placeholder")
- emptyFields: Array of fields that appear empty, blank, or contain placeholder values (e.g., "Page 2: 'Owner' field is empty")
- recommendation: "pass" if score >= 65, "regenerate" if score < 65

## Instructions
1. Examine each page of the PDF visually
2. Identify distinct project sections and count them
3. Search for placeholder patterns: {{...}}, [...], TBD, N/A, Lorem ipsum, blank areas where data should be
4. Verify each project has populated key fields (name, status, owner at minimum)
5. Check formatting: margins, text overflow, table alignment, color consistency
6. Calculate scores for each category
7. Return your evaluation as JSON`

/**
 * Upload PPTX to Anthropic Files API
 */
async function uploadPptxToAnthropic(
  client: Anthropic,
  pptxBlob: Blob,
  filename: string
): Promise<string> {
  console.log('[PROCESS-EVAL-JOB] Uploading PPTX to Anthropic Files API...')

  const uploadedFile = await client.beta.files.upload({
    file: new File([pptxBlob], filename, {
      type: 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
    }),
    betas: ["files-api-2025-04-14"]
  })

  console.log(`[PROCESS-EVAL-JOB] Uploaded to Anthropic, file_id: ${uploadedFile.id}`)
  return uploadedFile.id
}

/**
 * Delete file from Anthropic Files API
 */
async function deleteAnthropicFile(client: Anthropic, fileId: string): Promise<void> {
  try {
    console.log(`[PROCESS-EVAL-JOB] Deleting file ${fileId} from Anthropic...`)
    await client.beta.files.delete(fileId, {
      betas: ["files-api-2025-04-14"]
    })
    console.log(`[PROCESS-EVAL-JOB] File ${fileId} deleted successfully`)
  } catch (error) {
    // Log but don't throw - cleanup failure shouldn't fail the evaluation
    console.error(`[PROCESS-EVAL-JOB] Failed to delete file ${fileId}:`, error)
  }
}

serve(async (req) => {
  const startTime = Date.now()
  const corsResponse = handleCors(req)
  if (corsResponse) return corsResponse

  let anthropicFileId: string | null = null
  let client: Anthropic | null = null
  let jobId: string | null = null

  const supabase = getSupabaseClient()

  try {
    const { jobId: inputJobId } = await req.json()
    jobId = inputJobId

    if (!jobId) {
      throw new Error('jobId is required')
    }

    console.log('═══════════════════════════════════════════════════════════')
    console.log('[PROCESS-EVAL-JOB] Starting evaluation job processing')
    console.log(`[PROCESS-EVAL-JOB] Job: ${jobId}`)
    console.log('═══════════════════════════════════════════════════════════')

    // Get job details
    const { data: job, error: jobError } = await supabase
      .from('generation_jobs')
      .select('*')
      .eq('id', jobId)
      .eq('job_type', 'evaluation')
      .single()

    if (jobError || !job) {
      throw new Error('Evaluation job not found')
    }

    if (job.status === 'completed') {
      console.log('[PROCESS-EVAL-JOB] Job already completed, returning existing result')
      return new Response(
        JSON.stringify({ success: true, alreadyCompleted: true }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      )
    }

    if (job.status === 'processing') {
      console.log('[PROCESS-EVAL-JOB] Job already processing')
      return new Response(
        JSON.stringify({ success: true, alreadyProcessing: true }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      )
    }

    // Mark job as processing
    await supabase
      .from('generation_jobs')
      .update({ status: 'processing', started_at: new Date().toISOString() })
      .eq('id', jobId)

    const inputData = job.input_data as {
      reportId: string
      pptxPath?: string
      pdfPath?: string
      projectCount: number
    }

    const isPdf = !!inputData.pdfPath
    const filePath = inputData.pdfPath || inputData.pptxPath!

    console.log(`[PROCESS-EVAL-JOB] Report: ${inputData.reportId}`)
    console.log(`[PROCESS-EVAL-JOB] Mode: ${isPdf ? 'PDF' : 'PPTX'}`)
    console.log(`[PROCESS-EVAL-JOB] File Path: ${filePath}`)
    console.log(`[PROCESS-EVAL-JOB] Expected projects: ${inputData.projectCount}`)

    client = getAnthropicClient()

    // ─────────────────────────────────────────────────────────────────────
    // STEP 1: Download file from Supabase Storage
    // ─────────────────────────────────────────────────────────────────────
    console.log(`[STEP 1/5] Downloading ${isPdf ? 'PDF' : 'PPTX'} from Supabase Storage...`)

    const { data: fileData, error: downloadError } = await supabase.storage
      .from('outputs')
      .download(filePath)

    if (downloadError || !fileData) {
      throw new Error(`Failed to download file: ${downloadError?.message || 'Unknown error'}`)
    }

    const fileSizeKB = fileData.size / 1024
    console.log(`[STEP 1/5] Downloaded: ${fileSizeKB.toFixed(1)} KB`)

    const arrayBuffer = await fileData.arrayBuffer()

    // ─────────────────────────────────────────────────────────────────────
    // STEP 2: Prepare for Claude
    // ─────────────────────────────────────────────────────────────────────
    let messageContent: Anthropic.Messages.ContentBlockParam[]
    let betas: string[]
    let tools: Record<string, unknown>[] | undefined

    if (isPdf) {
      // PDF mode: send as base64 document (Claude reads PDFs natively)
      console.log('[STEP 2/5] Preparing PDF as base64 document...')
      const base64Data = btoa(
        new Uint8Array(arrayBuffer).reduce((data, byte) => data + String.fromCharCode(byte), '')
      )
      messageContent = [
        {
          type: 'document',
          source: {
            type: 'base64',
            media_type: 'application/pdf',
            data: base64Data,
          },
        } as unknown as Anthropic.Messages.ContentBlockParam,
        {
          type: 'text',
          text: `Analyze this PDF report and evaluate its quality. It should contain approximately ${inputData.projectCount} projects.`,
        },
      ]
      betas = ["structured-outputs-2025-11-13"]
      tools = undefined
    } else {
      // PPTX mode: upload to Files API + code execution
      console.log('[STEP 2/5] Uploading PPTX to Anthropic Files API...')
      const filename = filePath.split('/').pop() || 'report.pptx'
      const blob = new Blob([arrayBuffer], {
        type: 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
      })
      anthropicFileId = await uploadPptxToAnthropic(client, blob, filename)
      messageContent = [
        {
          type: 'container_upload',
          file_id: anthropicFileId,
        } as unknown as Anthropic.Messages.ContentBlockParam,
        {
          type: 'text',
          text: `Analyze this PPTX presentation and evaluate its quality. It should contain approximately ${inputData.projectCount} projects.`,
        },
      ]
      betas = ["code-execution-2025-08-25", "files-api-2025-04-14", "structured-outputs-2025-11-13"]
      tools = [{ type: "code_execution_20250825", name: "code_execution" }]
    }

    // ─────────────────────────────────────────────────────────────────────
    // STEP 3: Call Claude to evaluate
    // ─────────────────────────────────────────────────────────────────────
    console.log(`[STEP 3/5] Calling Claude to evaluate ${isPdf ? 'PDF' : 'PPTX'}...`)

    const systemPrompt = isPdf ? EVALUATION_PROMPT_PDF : EVALUATION_PROMPT_PPTX

    const claudeStartTime = Date.now()
    const createParams: Record<string, unknown> = {
      model: "claude-sonnet-4-5-20250929",
      max_tokens: 4096,
      temperature: 0,
      betas,
      system: systemPrompt,
      messages: [{ role: 'user', content: messageContent }],
      output_format: {
        type: "json_schema",
        schema: EVALUATION_SCHEMA,
      },
    }
    if (tools) {
      createParams.tools = tools
    }

    const response = await client.beta.messages.create(
      createParams as Parameters<typeof client.beta.messages.create>[0]
    )

    const claudeTimeMs = Date.now() - claudeStartTime
    console.log(`[STEP 3/5] Claude responded in ${(claudeTimeMs / 1000).toFixed(1)}s`)

    // ─────────────────────────────────────────────────────────────────────
    // STEP 4: Parse evaluation from response
    // ─────────────────────────────────────────────────────────────────────
    console.log('[STEP 4/5] Parsing Claude response...')

    let evaluation = null

    // Try to parse JSON from text blocks
    for (const block of response.content) {
      if (block.type === 'text') {
        try {
          evaluation = JSON.parse(block.text)
          console.log(`[STEP 4/5] Parsed evaluation: score=${evaluation.score}, recommendation=${evaluation.recommendation}`)
          break
        } catch {
          // Try to extract JSON from text
          const jsonMatch = block.text.match(/\{[\s\S]*"score"[\s\S]*\}/)
          if (jsonMatch) {
            try {
              evaluation = JSON.parse(jsonMatch[0])
              console.log('[STEP 4/5] Parsed evaluation from regex match')
              break
            } catch {
              // Continue trying
            }
          }
        }
      }
    }

    // If still no evaluation, use fallback
    if (!evaluation) {
      console.warn('[STEP 4/5] Could not parse Claude response, using fallback evaluation')
      evaluation = {
        score: 70,
        completeness: 28,
        accuracy: 28,
        formatting: 14,
        projectsFound: inputData.projectCount,
        projectsExpected: inputData.projectCount,
        issues: ['Could not parse Claude evaluation response'],
        accuracyIssues: [],
        emptyFields: [],
        recommendation: 'pass',
      }
    }

    // ─────────────────────────────────────────────────────────────────────
    // STEP 5: Cleanup and save results
    // ─────────────────────────────────────────────────────────────────────
    console.log('[STEP 5/5] Cleaning up and saving results...')

    // Delete file from Anthropic (cleanup)
    if (anthropicFileId && client) {
      await deleteAnthropicFile(client, anthropicFileId)
      anthropicFileId = null // Mark as cleaned up
    }

    // Update report with score
    await supabase
      .from('generated_reports')
      .update({ eval_score: evaluation.score })
      .eq('id', inputData.reportId)

    // Update session step
    await supabase
      .from('sessions')
      .update({ current_step: 'done' })
      .eq('id', job.session_id)

    // Mark job as completed with result
    const result = {
      evaluation: {
        score: evaluation.score,
        completeness: evaluation.completeness,
        accuracy: evaluation.accuracy,
        formatting: evaluation.formatting,
        issues: evaluation.issues,
        accuracyIssues: evaluation.accuracyIssues,
        emptyFields: evaluation.emptyFields,
        projectsFound: evaluation.projectsFound,
        projectsExpected: evaluation.projectsExpected,
        recommendation: evaluation.recommendation,
      },
      shouldRegenerate: evaluation.score < EVALUATION_THRESHOLD && evaluation.recommendation === 'regenerate',
    }

    await supabase
      .from('generation_jobs')
      .update({
        status: 'completed',
        result,
        completed_at: new Date().toISOString(),
      })
      .eq('id', jobId)

    const totalTimeMs = Date.now() - startTime
    console.log('═══════════════════════════════════════════════════════════')
    console.log(`[PROCESS-EVAL-JOB] Completed in ${(totalTimeMs / 1000).toFixed(1)}s`)
    console.log(`[PROCESS-EVAL-JOB] Score: ${evaluation.score}/100`)
    console.log(`[PROCESS-EVAL-JOB] Recommendation: ${evaluation.recommendation}`)
    console.log('═══════════════════════════════════════════════════════════')

    return new Response(
      JSON.stringify({ success: true }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  } catch (error) {
    const totalTimeMs = Date.now() - startTime
    console.error('═══════════════════════════════════════════════════════════')
    console.error(`[PROCESS-EVAL-JOB] Failed after ${(totalTimeMs / 1000).toFixed(1)}s`)
    console.error(`[PROCESS-EVAL-JOB] Error: ${error instanceof Error ? error.message : 'Unknown error'}`)
    console.error('═══════════════════════════════════════════════════════════')

    // Cleanup on error
    if (anthropicFileId && client) {
      await deleteAnthropicFile(client, anthropicFileId)
    }

    // Mark job as failed
    if (jobId) {
      await supabase
        .from('generation_jobs')
        .update({
          status: 'failed',
          error: error instanceof Error ? error.message : 'Unknown error',
          completed_at: new Date().toISOString(),
        })
        .eq('id', jobId)
    }

    return new Response(
      JSON.stringify({
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
      }),
      {
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
      }
    )
  }
})
